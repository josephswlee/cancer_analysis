---
title: "Cancer Analysis"
author: "Joseph Lee (sl5nj), Umar Abushaban (uba6z), William Cull (wjc5rt)"
date: "2/1/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
#Load libraries
#library(e1071)
library(tidyverse)
library(plotly)
library(htmltools)
library(devtools)
library(caret)
library(NbClust)
library(randomForest)
library(rio)
library(plyr)
library(rpart)
library(pROC)
library(rpart.plot)
library(rattle)
library(mlbench)
library(MLmetrics)
library(ROCR)
library(mltools)
library(data.table)
library(ggplot2)
library(class)
```

```{r}
setwd("/Users/sangwoolee/Dev/DS4002/4002_handsOn/project1/cancer_analysis")
```


Reading in the Data
```{r}
cancer = read.csv("./cancer.csv")
```

Cleaning up the data
```{r}
#convert variable names
cancer$diagnosis[cancer$diagnosis == "M"] <- 1
cancer$diagnosis[cancer$diagnosis == "B"] <- 0

#Remove the last "x" column
cancer = cancer[, -c(33)]
str(cancer)
#Convert the diagnosis variable to a factor
#complete.cases(cancer)
cancer$id = as.factor(cancer$id)
cancer$diagnosis = as.factor(cancer$diagnosis)
#View(cancer)
table(cancer$diagnosis)

#remove id for test
cancer = cancer[, -c(1)]
```

```{r}
sample_rows = 1:nrow(cancer)
#sample_rows

set.seed(1984) #sample(x, size, replace = FALSE, prob = NULL)
test_rows = sample(sample_rows,
                   dim(cancer)[1]*.10, #start with 10% of our dataset, could do 20%
                   # but random forest does require more training data because of the 
                   # sampling so 90% might be a better approach with this small of a dataset
                   replace = FALSE)# We don't want duplicate samples

# Partition the data between training and test sets using the row numbers the
# sample() function selected.
cancer_train = cancer[-test_rows,]
cancer_test = cancer[test_rows,]

dim(cancer)
```

```{r}
###### for tune

set.seed(1984) #sample(x, size, replace = FALSE, prob = NULL)
test_rows = sample(sample_rows,
                   dim(cancer_test)[1]*.50, #start with 10% of our dataset, could do 20%
                   # but random forest does require more training data because of the 
                   # sampling so 90% might be a better approach with this small of a dataset
                   replace = FALSE)# We don't want duplicate samples

tune = cancer_test[-test_rows,]
test = cancer_test[test_rows,]

set.seed(1984) #sample(x, size, replace = FALSE, prob = NULL)
test_rows = sample(sample_rows,
                   dim(tune)[1]*.50, #start with 10% of our dataset, could do 20%
                   # but random forest does require more training data because of the 
                   # sampling so 90% might be a better approach with this small of a dataset
                   replace = FALSE)# We don't want duplicate samples

x_tune = tune[-test_rows,]
y_tune = tune[test_rows,]
```

```{r}
mytry_tune <- function(x){
  xx <- dim(x)[2]-1
  sqrt(xx)
}
mytry_tune(cancer) #5.477226
```

Random Forest
```{r}
set.seed(2023)  
cancer_RF = randomForest((diagnosis)~.,          #<- Formula: response variable ~ predictors.
                            #   The period means 'use all other variables in the data'.
                            cancer_train,     #<- A data frame with the variables to be used.
                            #y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
                            #subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
                            #xtest = NULL,       #<- This is already defined in the formula by the ".".
                            #ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
                            ntree = 500,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
                            mtry = 5,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
                            replace = TRUE,      #<- Should sampled data points be replaced.
                            #classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population 
                            #strata = NULL,      #<- Not necessary for our purpose here.
                            sampsize = 100,      #<- Size of sample to draw each time.
                            nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
                            #maxnodes = NULL,    #<- Limits the number of maximum splits. 
                            importance = TRUE,   #<- Should importance of predictors be assessed?
                            #localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
                            proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
                            norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
                            do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
                            keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
                            keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees? 
```

```{r}
cancer_RF
cancer_RF$call
cancer_RF$confusion
```

```{r}
cancer_RF_acc = sum(cancer_RF$confusion[row(cancer_RF$confusion) == 
                                                col(cancer_RF$confusion)]) / 
  sum(cancer_RF$confusion)

cancer_RF_acc
```

```{r}
# The accuracy of this model is 0.8404

#### Random forest output ####

#View(as.data.frame(cancer_RF$votes))

# The "inbag" argument shows you which data point is included in which trees.
str(as.data.frame(cancer_RF$inbag))
```

```{r}
#View(as.data.frame(cancer_RF$inbag))

inbag <- as.data.frame(cancer_RF$inbag)

sum(inbag[,500])
```

```{r}
dim(cancer_RF$inbag)
```

```{r}
err.rate <- as.data.frame(cancer_RF$err.rate)
err.rate
```

```{r}
#### Visualize random forest results ####

# Let's visualize the results of the random forest.
# Let's start by looking at how the error rate changes as we add more trees.
cancer_RF_error = data.frame(1:nrow(cancer_RF$err.rate),
                                cancer_RF$err.rate)
View(cancer_RF_error)

colnames(cancer_RF_error) = c("Number of Trees", "Out of the Box",
                                 "Benign", "Malignant")

# Add another variable that measures the difference between the error rates, in
# some situations we would want to minimize this but need to use caution because
# it could be that the differences are small but that both errors are really high,
# just another point to track. 

cancer_RF_error$Diff <- cancer_RF_error$`Benign`-cancer_RF_error$`Malignant`

#View(cancer_RF_error)

#rm(fig)
fig <- plot_ly(x=cancer_RF_error$`Number of Trees`, y=cancer_RF_error$Diff,name="Diff", type = 'scatter', mode = 'lines')
fig <- fig %>% add_trace(y=cancer_RF_error$`Out of the Box`, name="OOB_Er")
fig <- fig %>% add_trace(y=cancer_RF_error$`Malignant`, name="Malignant")
fig <- fig %>% add_trace(y=cancer_RF_error$`Benign`, name="Benign")

fig
```

More random forest model

Modified ntree 500 -> 1000 and samplesize 100 -> 200
```{r}
set.seed(2023)  
cancer_RF2 = randomForest((diagnosis)~.,          #<- Formula: response variable ~ predictors.
                            #   The period means 'use all other variables in the data'.
                            cancer_train,     #<- A data frame with the variables to be used.
                            #y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
                            #subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
                            #xtest = NULL,       #<- This is already defined in the formula by the ".".
                            #ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
                            ntree = 1000,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
                            mtry = 5,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
                            replace = TRUE,      #<- Should sampled data points be replaced.
                            #classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population 
                            #strata = NULL,      #<- Not necessary for our purpose here.
                            sampsize = 200,      #<- Size of sample to draw each time.
                            nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
                            #maxnodes = NULL,    #<- Limits the number of maximum splits. 
                            importance = TRUE,   #<- Should importance of predictors be assessed?
                            #localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
                            proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
                            norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
                            do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
                            keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
                            keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees? 
```

```{r}
cancer_RF2
cancer_RF2$call
cancer_RF2$confusion
```

```{r}
cancer_RF_acc2 = sum(cancer_RF2$confusion[row(cancer_RF2$confusion) == 
                                                col(cancer_RF2$confusion)]) / 
  sum(cancer_RF2$confusion)

cancer_RF_acc2
```

```{r}
# The accuracy of this model is 0.8404

#### Random forest output ####

#View(as.data.frame(cancer_RF$votes))

# The "inbag" argument shows you which data point is included in which trees.
str(as.data.frame(cancer_RF2$inbag))
```

```{r}
#View(as.data.frame(cancer_RF2$inbag))

inbag <- as.data.frame(cancer_RF2$inbag)

sum(inbag[,500])
```

```{r}
dim(cancer_RF2$inbag)
```

```{r}
err.rate2 <- as.data.frame(cancer_RF2$err.rate)
err.rate2
```

```{r}
#### Visualize random forest results ####

# Let's visualize the results of the random forest.
# Let's start by looking at how the error rate changes as we add more trees.
cancer_RF_error2 = data.frame(1:nrow(cancer_RF2$err.rate),
                                cancer_RF2$err.rate)
#View(cancer_RF_error2)

colnames(cancer_RF_error2) = c("Number of Trees", "Out of the Box",
                                 "Benign", "Malignant")

# Add another variable that measures the difference between the error rates, in
# some situations we would want to minimize this but need to use caution because
# it could be that the differences are small but that both errors are really high,
# just another point to track. 

cancer_RF_error2$Diff <- cancer_RF_error2$`Benign`-cancer_RF_error2$`Malignant`

#View(cancer_RF_error)

#rm(fig)
fig2 <- plot_ly(x=cancer_RF_error2$`Number of Trees`, y=cancer_RF_error2$Diff,name="Diff", type = 'scatter', mode = 'lines')
fig2 <- fig2 %>% add_trace(y=cancer_RF_error2$`Out of the Box`, name="OOB_Er")
fig2 <- fig2 %>% add_trace(y=cancer_RF_error2$`Malignant`, name="Malignant")
fig2 <- fig2 %>% add_trace(y=cancer_RF_error2$`Benign`, name="Benign")

fig2
```

Graphing radius by diagnosis
```{r}
ggplot(cancer, aes(x=diagnosis, y=radius_mean,color = diagnosis)) +
  geom_point(alpha = 1/20) +
  geom_point(size=0.1) +
  labs(title = "Diagnosis by Radius", x="Diagnosis", y="Radius",) +
  scale_color_manual(labels = c("benign", "malignant"), values = c("green", "red")) +
  scale_x_discrete(labels=c("1" = "malignant", "0" = "benign"))
```





```{r}
set.seed(1891)
library(readr)
cancer_cor <- cancer
cancer_cor$diagnosis <- as.numeric(cancer_cor$diagnosis) 
View(cancer_cor)
```


```{r}
table(cancer$diagnosis)
summary(cancer[c("radius_mean","smoothness_mean","compactness_mean")])
```

```{r}
library(corrplot)
library(corrr)
library(ggplot2)
cor1 <- cor(x = cancer_cor$diagnosis, y = cancer_cor[2:19], use="complete.obs")
corrplot(cor1, tl.srt = 25)
```



```{r}
radius_d <- density(cancer$radius_mean)
plot(radius_d)
smoothness_d <- density(cancer$smoothness_mean)
plot(smoothness_d)
compactness_d <- density(cancer$compactness_mean)
plot(compactness_d)
```



#feature scaling
```{r}
# normalization must occur given different scales, kNN is distance based, and metrics are not gaussian in dist.
normalize <- function(x) {
  return ((x- min(x))/ (max(x)- min(x)))
}
cancer_norm <- as.data.frame(lapply(cancer_cor[2:19], normalize))
summary(cancer_norm$radius_mean)
# looks normalized
```

```{r}
# next comes training formation

cancer_train <- cancer_norm[1:469, ] 
cancer_test <- cancer_norm[470:569, ]  

cancer_train_labels <- cancer[1:469, 1]  
cancer_test_labels <- cancer[470:569, 1] 


```


```{r}

chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments#   If true, all distances equal to the kth largest are included
  conf_mat = table(class_knn, val_class)
  
  # Calculate the accuracy#could change this to Sensitivity 
  accu = sum(conf_mat[row(conf_mat) == col(conf_mat)]) / sum(conf_mat)                         
  cbind(k = k, accuracy = accu)
}

knn_diff_k_cancer <- sapply(seq(1,21, by = 2), function(x) chooseK(k = 18,
train_set = cancer_train,
val_set = cancer_test,
 train_class = cancer_train$`diagnosis`,
 val_class = cancer_test$`diagnosis`))

knn_diff_k_cancer
```


```{r}

cancer_test_pred <- knn(train = cancer_train, test = cancer_test,
cl = cancer_train_labels, k=18)

```


```{r}

library(e1071)

cancer_test_labels
cancer_train

cm18 <- confusionMatrix(
  cancer_test_pred, 
  cancer_train$`diagnosis`, 
  positive = "1", 
  dnn = c("Prediction", "Actual"), 
  mode = "sens_spec"
  )

cm18

```

```{r}

#Error in prediction
error <- mean(cancer_test_pred!=cancer$diagnosis)


```