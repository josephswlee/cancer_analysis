---
title: "Cancer Analysis"
author: "Joseph Lee (sl5nj), Umar Abushaban (uba6z), William Cull (wjc5rt)"
date: "2/1/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
#Load libraries
#library(e1071)
library(tidyverse)
library(plotly)
library(htmltools)
library(devtools)
library(caret)
library(NbClust)
library(randomForest)
library(rio)
library(plyr)
library(rpart)
library(pROC)
library(rpart.plot)
library(rattle)
library(mlbench)
library(MLmetrics)
library(ROCR)
library(mltools)
library(data.table)
library(ggplot2)
library(class)
```

```{r}
setwd("/Users/sangwoolee/Dev/DS4002/4002_handsOn/project1/cancer_analysis")
```


Reading in the Data
```{r}
cancer = read.csv("./cancer.csv")
```

Cleaning up the data
```{r}
#convert variable names
cancer$diagnosis[cancer$diagnosis == "M"] <- 1
cancer$diagnosis[cancer$diagnosis == "B"] <- 0

#Remove the last "x" column
cancer = cancer[, -c(33)]
str(cancer)
#Convert the diagnosis variable to a factor
#complete.cases(cancer)
cancer$id = as.factor(cancer$id)
cancer$diagnosis = as.factor(cancer$diagnosis)
#View(cancer)
table(cancer$diagnosis)

#remove id for test
cancer = cancer[, -c(1)]
```

```{r}
sample_rows = 1:nrow(cancer)
#sample_rows

set.seed(1984) #sample(x, size, replace = FALSE, prob = NULL)
test_rows = sample(sample_rows,
                   dim(cancer)[1]*.10, #start with 10% of our dataset, could do 20%
                   # but random forest does require more training data because of the 
                   # sampling so 90% might be a better approach with this small of a dataset
                   replace = FALSE)# We don't want duplicate samples

# Partition the data between training and test sets using the row numbers the
# sample() function selected.
cancer_train = cancer[-test_rows,]
cancer_test = cancer[test_rows,]

dim(cancer)
```

```{r}
###### for tune

set.seed(1984) #sample(x, size, replace = FALSE, prob = NULL)
test_rows = sample(sample_rows,
                   dim(cancer_test)[1]*.50, #start with 10% of our dataset, could do 20%
                   # but random forest does require more training data because of the 
                   # sampling so 90% might be a better approach with this small of a dataset
                   replace = FALSE)# We don't want duplicate samples

tune = cancer_test[-test_rows,]
test = cancer_test[test_rows,]

set.seed(1984) #sample(x, size, replace = FALSE, prob = NULL)
test_rows = sample(sample_rows,
                   dim(tune)[1]*.50, #start with 10% of our dataset, could do 20%
                   # but random forest does require more training data because of the 
                   # sampling so 90% might be a better approach with this small of a dataset
                   replace = FALSE)# We don't want duplicate samples

x_tune = tune[-test_rows,]
y_tune = tune[test_rows,]
```

```{r}
mytry_tune <- function(x){
  xx <- dim(x)[2]-1
  sqrt(xx)
}
mytry_tune(cancer) #5.477226
```

Random Forest
```{r}
set.seed(2023)  
cancer_RF = randomForest((diagnosis)~.,      
                            cancer_train,   
                            #y = NULL,        
                            #subset = NULL,    
                            #xtest = NULL,     
                            #ytest = NULL,     
                            ntree = 500,      
                            mtry = 5,      
                            replace = TRUE,   
                            #classwt = NULL,   
                            #strata = NULL,   
                            sampsize = 100,    
                            nodesize = 5,    
                            #maxnodes = NULL,  
                            importance = TRUE,  
                            #localImp = FALSE, 
                            proximity = FALSE,  
                            norm.votes = TRUE, 
                            do.trace = TRUE,   
                            keep.forest = TRUE, 
                            keep.inbag = TRUE)   
```

```{r}
cancer_RF
cancer_RF$call
cancer_RF$confusion
```

```{r}
cancer_RF_acc = sum(cancer_RF$confusion[row(cancer_RF$confusion) == 
                                                col(cancer_RF$confusion)]) / 
  sum(cancer_RF$confusion)

cancer_RF_acc
```

```{r}
# The accuracy of this model is 0.8404

#### Random forest output ####

#View(as.data.frame(cancer_RF$votes))

# The "inbag" argument shows you which data point is included in which trees.
str(as.data.frame(cancer_RF$inbag))
```

```{r}
#View(as.data.frame(cancer_RF$inbag))

inbag <- as.data.frame(cancer_RF$inbag)

sum(inbag[,500])
```

```{r}
dim(cancer_RF$inbag)
```

```{r}
err.rate <- as.data.frame(cancer_RF$err.rate)
err.rate
```

```{r}
#### Visualize random forest results ####

# Let's visualize the results of the random forest.
# Let's start by looking at how the error rate changes as we add more trees.
cancer_RF_error = data.frame(1:nrow(cancer_RF$err.rate),
                                cancer_RF$err.rate)
#View(cancer_RF_error)

colnames(cancer_RF_error) = c("Number of Trees", "Out of the Box",
                                 "Benign", "Malignant")

# Add another variable that measures the difference between the error rates, in
# some situations we would want to minimize this but need to use caution because
# it could be that the differences are small but that both errors are really high,
# just another point to track. 

cancer_RF_error$Diff <- cancer_RF_error$`Benign`-cancer_RF_error$`Malignant`

#View(cancer_RF_error)

#rm(fig)
fig <- plot_ly(x=cancer_RF_error$`Number of Trees`, y=cancer_RF_error$Diff,name="Diff", type = 'scatter', mode = 'lines')
fig <- fig %>% add_trace(y=cancer_RF_error$`Out of the Box`, name="OOB_Er")
fig <- fig %>% add_trace(y=cancer_RF_error$`Malignant`, name="Malignant")
fig <- fig %>% add_trace(y=cancer_RF_error$`Benign`, name="Benign")

fig
```

More random forest model

Modified ntree 500 -> 1000 and samplesize 100 -> 200
```{r}
set.seed(2023)  
cancer_RF2 = randomForest((diagnosis)~.,        
                            cancer_train, 
                            #y = NULL,      
                            #subset = NULL,   
                            #xtest = NULL,      
                            #ytest = NULL,   
                            ntree = 800,   
                            mtry = 5,     
                            replace = TRUE, 
                            #classwt = NULL, 
                            sampsize = 200,      
                            nodesize = 20,       
                            #maxnodes = NULL,  
                            importance = TRUE,   
                            #localImp = FALSE,  
                            proximity = FALSE, 
                            norm.votes = TRUE,  
                            do.trace = TRUE,    
                            keep.forest = TRUE, 
                            keep.inbag = TRUE)    
```

```{r}
cancer_RF2
cancer_RF2$call
cancer_RF2$confusion
```

```{r}
cancer_RF_acc2 = sum(cancer_RF2$confusion[row(cancer_RF2$confusion) == 
                                                col(cancer_RF2$confusion)]) / 
  sum(cancer_RF2$confusion)

cancer_RF_acc2
```

```{r}
# The accuracy of this model is 0.8404

#### Random forest output ####

#View(as.data.frame(cancer_RF$votes))

# The "inbag" argument shows you which data point is included in which trees.
str(as.data.frame(cancer_RF2$inbag))
```

```{r}
#View(as.data.frame(cancer_RF2$inbag))

inbag <- as.data.frame(cancer_RF2$inbag)

sum(inbag[,500])
```

```{r}
dim(cancer_RF2$inbag)
```

```{r}
err.rate2 <- as.data.frame(cancer_RF2$err.rate)
err.rate2
```

```{r}
#### Visualize random forest results ####

# Let's visualize the results of the random forest.
# Let's start by looking at how the error rate changes as we add more trees.
cancer_RF_error2 = data.frame(1:nrow(cancer_RF2$err.rate),
                                cancer_RF2$err.rate)
#View(cancer_RF_error2)

colnames(cancer_RF_error2) = c("Number of Trees", "Out of the Box",
                                 "Benign", "Malignant")

# Add another variable that measures the difference between the error rates, in
# some situations we would want to minimize this but need to use caution because
# it could be that the differences are small but that both errors are really high,
# just another point to track. 

cancer_RF_error2$Diff <- cancer_RF_error2$`Benign`-cancer_RF_error2$`Malignant`

#View(cancer_RF_error)

#rm(fig)
fig2 <- plot_ly(x=cancer_RF_error2$`Number of Trees`, y=cancer_RF_error2$Diff,name="Diff", type = 'scatter', mode = 'lines')
fig2 <- fig2 %>% add_trace(y=cancer_RF_error2$`Out of the Box`, name="OOB_Er")
fig2 <- fig2 %>% add_trace(y=cancer_RF_error2$`Malignant`, name="Malignant")
fig2 <- fig2 %>% add_trace(y=cancer_RF_error2$`Benign`, name="Benign")

fig2
```

Variable Importance
```{r}
varImpPlot(cancer_RF2, 
           sort = TRUE,
           n.var = 10,        
           main = "Important Factors for diagnosis",
           #cex = 2,           
           bg = "white",       
           color = "blue",     
           lcolor = "orange")  
```

Evaluating Model
```{r}
cancer_predict = predict(cancer_RF2,    
                            cancer_test,      
                            type = "response",   
                            predict.all = TRUE, 
                            proximity = FALSE)  
#cancer_predict
```

```{r}
cancer_test_pred = data.frame(cancer_test, 
                                 Prediction = cancer_predict$aggregate)



# Create the confusion matrix.
cancer_test_matrix_RF = table(cancer_test_pred$diagnosis, 
                            cancer_test_pred$Prediction)

cancer_test_matrix_RF
```

```{r}
# Calculate the misclassification or the error rate.
cancer_test_error_rate_RF = sum(cancer_test_matrix_RF[row(cancer_test_matrix_RF) != 
                                                    col(cancer_test_matrix_RF)]) / 
  sum(cancer_test_matrix_RF)

cancer_test_error_rate_RF
```


```{r}
#   0  1
#0 34  0
#1  1 21

# False Positive Rate
1/(1+34)
```

```{r}
confusionMatrix(cancer_test_pred$Prediction,cancer_test_pred$diagnosis,positive = "1", 
                dnn=c("Prediction", "Actual"), mode = "everything")
```


Graphing radius by diagnosis
```{r}
ggplot(cancer, aes(x=diagnosis, y=radius_mean,color = diagnosis)) +
  geom_point(alpha = 1/20) +
  geom_point(size=0.1) +
  labs(title = "Diagnosis by Radius", x="Diagnosis", y="Radius",) +
  scale_color_manual(labels = c("benign", "malignant"), values = c("green", "red")) +
  scale_x_discrete(labels=c("1" = "malignant", "0" = "benign"))
```





```{r}
set.seed(1891)
library(readr)
cancer_cor <- cancer
cancer_cor$diagnosis <- as.numeric(cancer_cor$diagnosis) 
View(cancer_cor)
```


```{r}
table(cancer$diagnosis)
summary(cancer[c("radius_mean","smoothness_mean","compactness_mean")])
```

```{r}
library(corrplot)
library(corrr)
library(ggplot2)
cor1 <- cor(x = cancer_cor$diagnosis, y = cancer_cor[2:19], use="complete.obs")
corrplot(cor1, tl.srt = 25)
```



```{r}
radius_d <- density(cancer$radius_mean)
plot(radius_d)
smoothness_d <- density(cancer$smoothness_mean)
plot(smoothness_d)
compactness_d <- density(cancer$compactness_mean)
plot(compactness_d)
```



#feature scaling
```{r}
# normalization must occur given different scales, kNN is distance based, and metrics are not gaussian in dist.
normalize <- function(x) {
  return ((x- min(x))/ (max(x)- min(x)))
}
cancer_norm <- as.data.frame(lapply(cancer_cor[2:19], normalize))
summary(cancer_norm$radius_mean)
# looks normalized
```

```{r}
# next comes training formation

cancer_train <- cancer_norm[1:469, ] 
cancer_test <- cancer_norm[470:569, ]  

cancer_train_labels <- cancer[1:469, 1]  
cancer_test_labels <- cancer[470:569, 1] 


```


```{r}

chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments#   If true, all distances equal to the kth largest are included
  conf_mat = table(class_knn, val_class)
  
  # Calculate the accuracy#could change this to Sensitivity 
  accu = sum(conf_mat[row(conf_mat) == col(conf_mat)]) / sum(conf_mat)                         
  cbind(k = k, accuracy = accu)
}

knn_diff_k_cancer <- sapply(seq(1,21, by = 2), function(x) chooseK(k = 18,
train_set = cancer_train,
val_set = cancer_test,
 train_class = cancer_train$`diagnosis`,
 val_class = cancer_test$`diagnosis`))

knn_diff_k_cancer
```


```{r}

cancer_test_pred <- knn(train = cancer_train, test = cancer_test,
cl = cancer_train_labels, k=18)

```


```{r}

library(e1071)

cancer_test_labels
cancer_train

cm18 <- confusionMatrix(
  cancer_test_pred, 
  cancer_train$`diagnosis`, 
  positive = "1", 
  dnn = c("Prediction", "Actual"), 
  mode = "sens_spec"
  )

cm18

```

```{r}

#Error in prediction
error <- mean(cancer_test_pred!=cancer$diagnosis)


```